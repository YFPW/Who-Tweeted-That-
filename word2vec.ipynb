{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZvFcQ83l1s9R","colab_type":"text"},"source":["Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"wy4xosdMag1i","colab_type":"code","outputId":"27df8876-1111-410b-c251-0ea4cecfd9c1","executionInfo":{"status":"ok","timestamp":1568074914321,"user_tz":-600,"elapsed":1883662,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":423}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ls \"/content/gdrive/My Drive/SML_Project1\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"," checkpoint\n","'Copy of tokenizer.ipynb'\n"," Data.ipynb\n"," mlp2.hdf5\n"," mlp3.hdf5\n"," mlp3.png\n"," mlp4.hdf5\n"," mlp4.png\n"," mlp.hdf5\n","'Pre-trained BERT contextualized word embeddings.ipynb'\n"," pridicted.csv\n"," pridicted.gsheet\n"," project1.ipynb\n"," test_tweets_unlabeled.txt\n"," tokenizer.ipynb\n"," train_tweets.txt\n"," wwm_uncased_L-24_H-1024_A-16\n"," x_train_encode.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XPAqULeq1woI","colab_type":"text"},"source":["Lemmatization"]},{"cell_type":"code","metadata":{"id":"gpuiBLUKPF5C","colab_type":"code","outputId":"332f0f24-e7dd-4edc-9328-2a0fe032adea","executionInfo":{"status":"ok","timestamp":1568074915888,"user_tz":-600,"elapsed":1885217,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('wordnet')\n","lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n","\n","def lemmatize(word):\n","    lemma = lemmatizer.lemmatize(word,'v')\n","    if lemma == word:\n","        lemma = lemmatizer.lemmatize(word,'n')\n","    return lemma\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CeG5nsOaELH","colab_type":"text"},"source":["Tokenizer and extract features"]},{"cell_type":"code","metadata":{"id":"In-VryyqPL8Z","colab_type":"code","colab":{}},"source":["import re\n","from nltk.stem.porter import *\n","\n","rt_str = r'RT'\n","capital_str = r'[A-Z]'\n","mention_str = r'@handle(:)?'\n","emoticons_str = r'[:=;][oO\\-]?[D\\)\\]\\(\\]/\\\\OpP]'         \n","http_str = r'([-|:] )?http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n","#hashtag_str = r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w]+)\"\n","hashtag_str = r\"#\"\n","money_str = r\"$[0-9]+\"\n","percentage_str  = r'[0-9]+%'\n","\n","feature_regex_strs = [\n","    http_str,\n","    rt_str,\n","    emoticons_str,\n","    mention_str,\n","    hashtag_str,\n","    money_str,\n","    percentage_str\n","]\n","\n","punctuation_strs = [\n","    r'\\.',\n","    r',',\n","    r'!',\n","    r'\\?',\n","    r':',\n","    r';',\n","    r'\\'',\n","    r'\\\"',\n","    r'<[^>]+>',\n","    r'\\([^>]+\\)',\n","    r'\\[[^>]+\\]',\n","    r'\\{[^>]+\\}',\n","    r'[\\^|\\*|_|\\-|=|\\+|\\/|\\\\|\\||`|~|&]'\n","]\n","\n","feature_regex_strs.extend(punctuation_strs)\n","\n","class Tokenizer():\n","    \"\"\"\n","    This class is used to tokenize the tweets and calculate the average length.\n","    feature vector: number of url, number of 'RT', number of emotions, number of @, number of hashtag, number of cash mentioned, number of percentage, fraction of capitals\n","    \"\"\"\n","    def __init__(self):\n","        self.processed_length = 0\n","        self.processed_item = 1\n","        self.features = []\n","        self.max_seq = 0\n","        self.vocab = {}\n","        \n","    def num_of_match(self, patten, tweet):\n","        return len(re.findall(patten, tweet))\n","\n","    def tokenize(self, tweets):\n","        \"\"\"\n","        :param tweets: One tweet\n","        :return: tokens: tokenized tweet\n","                 feature array including captical number and @ number\n","        \"\"\"\n","        \n","        other_features = []\n","        for feature_regex_str in feature_regex_strs:\n","            other_features.append(self.num_of_match(feature_regex_str, tweets))\n","            tweets = re.sub(feature_regex_str, '', tweets)\n","        \n","        other_features.append(float(self.num_of_match(capital_str, tweets)) / len(tweets))\n","        \n","        self.features.append(other_features)\n","\n","        regex_str = [\n","            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # number\n","            r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # word with - and '\n","            r'(?:[\\w_]+)',\n","            r'(?:\\S)'\n","        ]\n","\n","        tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n","\n","        tokens = tokens_re.findall(tweets)\n","        tokens = [token.lower() if token.isalpha() else token for token in tokens]\n","        tokens = [lemmatize(token) if token.isalpha() else token for token in tokens]\n","        \n","        token_length = 0\n","        for token in tokens:\n","            token_length += len(token)\n","            if token not in self.vocab:\n","                self.vocab[token] = len(self.vocab)\n","        self.processed_length += token_length\n","        self.processed_item += 1\n","        self.max_seq = max(self.max_seq, len(tokens))\n","        return tokens\n","      \n","    def avg_length(self):\n","        \"\"\"\n","        :return: Average length of tokens\n","        \"\"\"\n","        return float(self.processed_length) / self.processed_item \n","      \n","    def get_other_features(self):\n","        return self.features\n","      \n","    def get_max_seq(self):\n","        return self.max_seq\n","    \n","    def get_vocab(self):\n","        return self.vocab\n","      \n","mytokenizer = Tokenizer()\n","\n","#tweet_example = 'RT @handle: Cool SEO \\'post\\' by @handle ! :) #RRPP #PR RT @handle: Top 10 #SEO Tips ? for #Public Relations - http://ow.ly/Bh7L'\n","#tweet_example2 = 'RT @handle: Note to webmasters: <the full roll> out of Caffeine won\\'t happen until after the holidays. More info: http://bit.ly/4GELv6s'\n","#tokens_example = mytokenizer.tokenize(tweet_example)\n","#tokens_example2 = mytokenizer.tokenize(tweet_example2)\n","#print(tokens_example)\n","#print(tokens_example2)\n","#print(mytokenizer.get_other_features())\n","#print(len(mytokenizer.get_other_features()[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pk8OZ0hlaGva","colab_type":"text"},"source":["Read data"]},{"cell_type":"code","metadata":{"id":"swwE1I3GaJeL","colab_type":"code","outputId":"f642cb8c-246d-4ff9-e230-96121ec1efbf","executionInfo":{"status":"ok","timestamp":1568074918025,"user_tz":-600,"elapsed":1887337,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["from sklearn.model_selection import train_test_split\n","\n","X = []\n","Y = []\n","with open ('/content/gdrive/My Drive/SML_Project1/train_tweets.txt') as fp:\n","    for line in fp:\n","        data = line.split(\"\\t\")\n","        X.append(data[1])\n","        Y.append(int(data[0]))\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=90051)\n","\n","real_to_label = {}\n","label_to_real = {}\n","Y_train_label = []\n","Y_test_label = []\n","for user in Y_train:\n","    if user not in real_to_label.keys():\n","        new_label = len(real_to_label)\n","        real_to_label[user] = len(real_to_label)\n","        label_to_real[new_label] = user\n","    Y_train_label.append(real_to_label[user])\n","for user in Y_test:\n","    if user not in real_to_label.keys():\n","        new_label = len(real_to_label)\n","        real_to_label[user] = len(real_to_label)\n","        label_to_real[new_label] = user\n","    Y_test_label.append(real_to_label[user])\n","\n","X_final_test = []\n","with open ('/content/gdrive/My Drive/SML_Project1/test_tweets_unlabeled.txt') as fp:\n","    for line in fp:\n","        X_final_test.append(line)\n","    \n","print(len(X_train))\n","print(len(Y_train))\n","print(len(Y_train_label))\n","print(len(X_test))\n","print(len(Y_test))\n","print(len(Y_test_label))\n","print(len(X_final_test))\n","print(len(real_to_label))\n","print(len(label_to_real))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["296038\n","296038\n","296038\n","32894\n","32894\n","32894\n","35437\n","9297\n","9297\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-juY8t7Lak7s","colab_type":"text"},"source":["Tokenize X_train, X_test and X_final_test"]},{"cell_type":"code","metadata":{"id":"dSyQdbIvatM9","colab_type":"code","outputId":"81daac3c-db07-4d43-cdd0-f04d862d994b","executionInfo":{"status":"ok","timestamp":1568074975740,"user_tz":-600,"elapsed":1945045,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["import numpy as np\n","\n","X_train_tokens = []\n","X_test_tokens = []\n","X_final_test_tokens = []\n","\n","for tweet in X_train:\n","    X_train_tokens.append(mytokenizer.tokenize(tweet))\n","for tweet in X_test:\n","    X_test_tokens.append(mytokenizer.tokenize(tweet))\n","for tweet in X_final_test:\n","    X_final_test_tokens.append(mytokenizer.tokenize(tweet))\n","    \n","other_features = np.array(mytokenizer.get_other_features())\n","additional_info = other_features[:len(X_train_tokens), :]\n","additional_info_val = other_features[len(X_train_tokens) : len(X_train_tokens) + len(X_test_tokens), :]\n","additional_info_test = other_features[-len(X_final_test_tokens) : , :]\n","print(len(other_features))\n","print(len(additional_info))\n","print(len(additional_info[0]))\n","print(len(additional_info_val))\n","print(len(additional_info_val[0]))\n","print(len(additional_info_test))\n","print(len(additional_info_test[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["364369\n","296038\n","21\n","32894\n","21\n","35437\n","21\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wnywomdWbv8i","colab_type":"text"},"source":["Construct vocab and word2vec model"]},{"cell_type":"code","metadata":{"id":"78LgKKPQb0Xh","colab_type":"code","outputId":"cae74948-bff3-4e5d-e08b-92dfaf82b0fd","executionInfo":{"status":"ok","timestamp":1568075000821,"user_tz":-600,"elapsed":1970115,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from gensim.models import Word2Vec\n","dimension = 100\n","vocab = mytokenizer.get_vocab()\n","print('<unk>' in vocab)\n","print(len(vocab))\n","common_texts = []\n","common_texts.extend(X_train_tokens)\n","common_texts.extend(X_test_tokens)\n","common_texts.extend(X_final_test_tokens)\n","w2v_model = Word2Vec(common_texts, size=dimension, window=5, min_count=1)\n","# print(w2v_model['top'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["False\n","158974\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a5BSoDW_kCTI","colab_type":"text"},"source":["Embedding metrix"]},{"cell_type":"code","metadata":{"id":"FOtf0sHWkCzu","colab_type":"code","outputId":"d405aa5d-12d7-4423-8056-a4b93c8fa447","executionInfo":{"status":"ok","timestamp":1568075001153,"user_tz":-600,"elapsed":1970436,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["embedding_matrix = np.zeros((len(vocab) + 1, dimension))\n","for word, i in vocab.items():\n","    try:\n","        embedding_vector = w2v_model[str(word)]\n","        embedding_matrix[i] = embedding_vector\n","    except KeyError:\n","        continue\n","print(embedding_matrix.shape)\n","# print(embedding_matrix[0])\n","# print(embedding_matrix[-1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["(158975, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c99a7DubmMmi","colab_type":"text"},"source":["padding"]},{"cell_type":"code","metadata":{"id":"689BfNbymOGv","colab_type":"code","outputId":"6a6f52c6-2888-4ad8-aa16-0120a3fcf054","executionInfo":{"status":"ok","timestamp":1568075006639,"user_tz":-600,"elapsed":1975904,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["from keras.preprocessing.sequence import pad_sequences\n","max_length = mytokenizer.get_max_seq()\n","print(max_length)\n","padding = '<unk>'\n","vocab[padding] = len(vocab)\n","print(len(vocab))\n","X_train_tokens = pad_sequences(X_train_tokens, dtype=object, maxlen=max_length, value=padding)\n","X_test_tokens = pad_sequences(X_test_tokens, dtype=object, maxlen=max_length, value=padding)\n","X_final_test_tokens = pad_sequences(X_final_test_tokens, dtype=object, maxlen=max_length, value=padding)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["118\n","158975\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VbAyYdh90RmR","colab_type":"text"},"source":["token to sequence"]},{"cell_type":"code","metadata":{"id":"v9be7xdn0YcL","colab_type":"code","colab":{}},"source":["def token2seq(tokens_list):\n","    seqs_list = []\n","    for tokens in tokens_list:\n","        seqs = []\n","        for token in tokens:\n","            seqs.append(vocab[token])\n","        seqs_list.append(seqs)\n","    return seqs_list\n","X_train_tokens = token2seq(X_train_tokens)\n","X_test_tokens = token2seq(X_test_tokens)\n","X_final_test_tokens = token2seq(X_final_test_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1X0M_ObpqZ1c","colab_type":"text"},"source":["Picture of training"]},{"cell_type":"code","metadata":{"id":"JdLHBPE4qc8w","colab_type":"code","colab":{}},"source":["from keras.callbacks import Callback\n","import matplotlib.pyplot as plt\n","import matplotlib\n","matplotlib.use('Agg')\n","\n","class LossHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('acc'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_acc'))\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.losses['epoch'].append(logs.get('loss'))\n","        self.accuracy['epoch'].append(logs.get('acc'))\n","        self.val_loss['epoch'].append(logs.get('val_loss'))\n","        self.val_acc['epoch'].append(logs.get('val_acc'))\n","\n","    def loss_plot(self, loss_type, savepath):\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        # val_acc\n","        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","        # val_loss\n","        plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc-loss')\n","        plt.legend(loc=\"upper right\")    \n","        plt.savefig(savepath)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIzNHiui9z2c","colab_type":"text"},"source":["label data"]},{"cell_type":"code","metadata":{"id":"KtFCPve790eu","colab_type":"code","colab":{}},"source":["import keras\n","num_classes = len(label_to_real)\n","Y_train_label = keras.utils.to_categorical(Y_train_label, num_classes=num_classes)  # to one-hot metrics\n","Y_test_label = keras.utils.to_categorical(Y_test_label, num_classes=num_classes)\n","history = LossHistory()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wxrgkCQ6jwjK","colab_type":"text"},"source":["Text CNN"]},{"cell_type":"code","metadata":{"id":"uPF8rtqcjyFk","colab_type":"code","outputId":"f95c1227-dbb8-4b6d-9226-94c912f37e57","colab":{"base_uri":"https://localhost:8080/","height":689}},"source":["from keras.models import Model\n","from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout, concatenate, Dense, Activation\n","from keras.optimizers import SGD, Adam\n","from keras import regularizers\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","print(len(vocab))\n","\n","def TextCNN(auxilury_input_size = (2, ), num_classes = 10001):\n","    \n","    auxilury_input = Input(shape=auxilury_input_size)\n","    main_input = Input(shape=(max_length, ), dtype='float64')\n","    \n","    embed = Embedding(len(vocab), dimension, input_length=max_length, weights=[embedding_matrix], trainable=False)(main_input)\n","\n","    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n","    cnn1 = MaxPooling1D(pool_size=38)(cnn1)\n","    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n","    cnn2 = MaxPooling1D(pool_size=37)(cnn2)\n","    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n","    cnn3 = MaxPooling1D(pool_size=36)(cnn3)\n","\n","    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n","    flat = Flatten()(cnn)\n","#     drop = Dropout(0.2)(flat)\n","    \n","    all_input = concatenate([flat, auxilury_input], axis=-1)\n","    main_output = Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(1e-3))(all_input)\n","\n","    model = Model(inputs=[main_input, auxilury_input], outputs=main_output)\n","    sgd = SGD(lr=0.01, decay=1e-8, momentum=0.9, nesterov=True)\n","    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n","    \n","    return model\n"," \n","model_path = '/content/gdrive/My Drive/SML_Project1/mlp5.hdf5'\n","result_image_path = '/content/gdrive/My Drive/SML_Project1/mlp5.png'\n","auxilury_input_size = (len(additional_info[0]),)\n","\n","model = TextCNN(auxilury_input_size = auxilury_input_size, num_classes = num_classes)\n","model_checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True)\n","model.fit([X_train_tokens, additional_info], Y_train_label, batch_size=16, epochs=20, validation_data=([X_test_tokens, additional_info_val], Y_test_label), \n","          callbacks=[model_checkpoint, EarlyStopping(monitor=\"val_acc\", patience=3), history])\n","history.loss_plot('epoch', result_image_path)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["158975\n","Train on 296038 samples, validate on 32894 samples\n","Epoch 1/20\n","296038/296038 [==============================] - 356s 1ms/step - loss: 9.2849 - acc: 0.0286 - val_loss: 8.7201 - val_acc: 0.0430\n","\n","Epoch 00001: loss improved from inf to 9.28489, saving model to /content/gdrive/My Drive/SML_Project1/mlp5.hdf5\n","Epoch 2/20\n","296038/296038 [==============================] - 350s 1ms/step - loss: 8.7118 - acc: 0.0477 - val_loss: 8.7739 - val_acc: 0.0473\n","\n","Epoch 00002: loss improved from 9.28489 to 8.71183, saving model to /content/gdrive/My Drive/SML_Project1/mlp5.hdf5\n","Epoch 3/20\n","296038/296038 [==============================] - 347s 1ms/step - loss: 8.7240 - acc: 0.0551 - val_loss: 8.7918 - val_acc: 0.0522\n","\n","Epoch 00003: loss did not improve from 8.71183\n","Epoch 4/20\n","296038/296038 [==============================] - 346s 1ms/step - loss: 8.7344 - acc: 0.0589 - val_loss: 8.8057 - val_acc: 0.0586\n","\n","Epoch 00004: loss did not improve from 8.71183\n","Epoch 5/20\n","296038/296038 [==============================] - 346s 1ms/step - loss: 8.7402 - acc: 0.0631 - val_loss: 8.8296 - val_acc: 0.0596\n","\n","Epoch 00005: loss did not improve from 8.71183\n","Epoch 6/20\n","296038/296038 [==============================] - 347s 1ms/step - loss: 8.7485 - acc: 0.0661 - val_loss: 8.8501 - val_acc: 0.0612\n","\n","Epoch 00006: loss did not improve from 8.71183\n","Epoch 7/20\n","296038/296038 [==============================] - 347s 1ms/step - loss: 8.7522 - acc: 0.0687 - val_loss: 8.8764 - val_acc: 0.0610\n","\n","Epoch 00007: loss did not improve from 8.71183\n","Epoch 8/20\n","296038/296038 [==============================] - 347s 1ms/step - loss: 8.7575 - acc: 0.0704 - val_loss: 8.9022 - val_acc: 0.0619\n","\n","Epoch 00008: loss did not improve from 8.71183\n","Epoch 9/20\n","296038/296038 [==============================] - 346s 1ms/step - loss: 8.7647 - acc: 0.0727 - val_loss: 8.9042 - val_acc: 0.0637\n","\n","Epoch 00009: loss did not improve from 8.71183\n","Epoch 10/20\n","126832/296038 [===========>..................] - ETA: 3:12 - loss: 8.6842 - acc: 0.0779Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zNlMMntx2CdB","colab_type":"code","colab":{}},"source":["import pandas as pd\n","output_file = \"/content/gdrive/My Drive/SML_Project1/pridicted.csv\"\n","result = model.predict([X_final_test_tokens, additional_info_test], verbose=1)\n","test_users = []\n","for res in result:\n","    t_list = res.tolist()\n","    test_users.append(label_to_real[t_list.index(max(t_list))])\n","df = pd.DataFrame({'Id': np.arange(1, len(test_users) + 1),\n","                  'Predicted' : np.array(test_users)})\n","print(df)\n","df.to_csv(output_file,index=False)"],"execution_count":0,"outputs":[]}]}