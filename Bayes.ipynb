{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bayes.ipynb","version":"0.3.2","provenance":[{"file_id":"1rcF3m-64b3GuIE74Aa4kgY8evAAFDXe2","timestamp":1568267405025}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZvFcQ83l1s9R","colab_type":"text"},"source":["Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"wy4xosdMag1i","colab_type":"code","outputId":"c412cb87-1a9d-4f6e-e1b9-d2fb7960798a","executionInfo":{"status":"ok","timestamp":1568268996032,"user_tz":-600,"elapsed":4342,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":420}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ls \"/content/gdrive/My Drive/SML_Project1\""],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"," checkpoint\n","'Copy of tokenizer.ipynb'\n"," Data.ipynb\n"," mlp2.hdf5\n"," mlp3.hdf5\n"," mlp3.png\n"," mlp4.hdf5\n"," mlp4.png\n"," mlp5.hdf5\n"," mlp.hdf5\n"," mlp_reg.hdf5\n"," mlp_sgd_30+.png\n"," mlp_sgd.hdf5\n"," mlp_sgd.png\n","'Pre-trained BERT contextualized word embeddings.ipynb'\n"," pridicted.csv\n"," pridicted.gsheet\n"," project1.ipynb\n"," test_tweets_unlabeled.txt\n"," train_tweets.txt\n"," word2vec.ipynb\n"," wwm_uncased_L-24_H-1024_A-16\n"," x_train_encode.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XPAqULeq1woI","colab_type":"text"},"source":["Lemmatization"]},{"cell_type":"code","metadata":{"id":"gpuiBLUKPF5C","colab_type":"code","outputId":"c46e7fc8-1ccb-4144-b80d-23ca9b94cc00","executionInfo":{"status":"ok","timestamp":1568269429393,"user_tz":-600,"elapsed":945,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('wordnet')\n","lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n","\n","def lemmatize(word):\n","    lemma = lemmatizer.lemmatize(word,'v')\n","    if lemma == word:\n","        lemma = lemmatizer.lemmatize(word,'n')\n","    return lemma\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CeG5nsOaELH","colab_type":"text"},"source":["Tokenizer and extract features"]},{"cell_type":"code","metadata":{"id":"In-VryyqPL8Z","colab_type":"code","colab":{}},"source":["import re\n","from nltk.stem.porter import *\n","\n","rt_str = r'RT'\n","capital_str = r'[A-Z]'\n","mention_str = r'@handle(:)?'\n","emoticons_str = r'[:=;][oO\\-]?[D\\)\\]\\(\\]/\\\\OpP]'         \n","http_str = r'([-|:] )?http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n","#hashtag_str = r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w]+)\"\n","hashtag_str = r\"#\"\n","money_str = r\"$[0-9]+\"\n","percentage_str  = r'[0-9]+%'\n","\n","feature_regex_strs = [\n","    http_str,\n","    rt_str,\n","    emoticons_str,\n","    mention_str,\n","    hashtag_str,\n","    money_str,\n","    percentage_str\n","]\n","\n","punctuation_strs = [\n","    r'\\.',\n","    r',',\n","    r'!',\n","    r'\\?',\n","    r':',\n","    r';',\n","    r'\\'',\n","    r'\\\"',\n","    r'<[^>]+>',\n","    r'\\([^>]+\\)',\n","    r'\\[[^>]+\\]',\n","    r'\\{[^>]+\\}',\n","    r'[\\^|\\*|_|\\-|=|\\+|\\/|\\\\|\\||`|~|&]'\n","]\n","\n","feature_regex_strs.extend(punctuation_strs)\n","\n","class Tokenizer():\n","    \"\"\"\n","    This class is used to tokenize the tweets and calculate the average length.\n","    feature vector: number of url, number of 'RT', number of emotions, number of @, number of hashtag, number of cash mentioned, number of percentage, fraction of capitals\n","    \"\"\"\n","    def __init__(self):\n","        self.processed_length = 0\n","        self.processed_item = 1\n","        self.features = []\n","        self.max_seq = 0\n","        self.vocab = {}\n","        \n","    def num_of_match(self, patten, tweet):\n","        return len(re.findall(patten, tweet))\n","\n","    def tokenize(self, tweets):\n","        \"\"\"\n","        :param tweets: One tweet\n","        :return: tokens: tokenized tweet\n","                 feature array including captical number and @ number\n","        \"\"\"\n","        \n","        other_features = []\n","        for feature_regex_str in feature_regex_strs:\n","            other_features.append(self.num_of_match(feature_regex_str, tweets))\n","            tweets = re.sub(feature_regex_str, '', tweets)\n","        \n","        other_features.append(float(self.num_of_match(capital_str, tweets)) / len(tweets))\n","        \n","        self.features.append(other_features)\n","\n","        regex_str = [\n","            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # number\n","            r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # word with - and '\n","            r'(?:[\\w_]+)',\n","            r'(?:\\S)'\n","        ]\n","\n","        tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n","\n","        tokens = tokens_re.findall(tweets)\n","        tokens = [token.lower() if token.isalpha() else token for token in tokens]\n","        tokens = [lemmatize(token) if token.isalpha() else token for token in tokens]\n","        \n","        token_length = 0\n","        for token in tokens:\n","            token_length += len(token)\n","            if token not in self.vocab:\n","                self.vocab[token] = len(self.vocab)\n","        self.processed_length += token_length\n","        self.processed_item += 1\n","        self.max_seq = max(self.max_seq, len(tokens))\n","        return tokens\n","      \n","    def avg_length(self):\n","        \"\"\"\n","        :return: Average length of tokens\n","        \"\"\"\n","        return float(self.processed_length) / self.processed_item \n","      \n","    def get_other_features(self):\n","        return self.features\n","      \n","    def get_max_seq(self):\n","        return self.max_seq\n","    \n","    def get_vocab(self):\n","        return self.vocab\n","      \n","mytokenizer = Tokenizer()\n","\n","#tweet_example = 'RT @handle: Cool SEO \\'post\\' by @handle ! :) #RRPP #PR RT @handle: Top 10 #SEO Tips ? for #Public Relations - http://ow.ly/Bh7L'\n","#tweet_example2 = 'RT @handle: Note to webmasters: <the full roll> out of Caffeine won\\'t happen until after the holidays. More info: http://bit.ly/4GELv6s'\n","#tokens_example = mytokenizer.tokenize(tweet_example)\n","#tokens_example2 = mytokenizer.tokenize(tweet_example2)\n","#print(tokens_example)\n","#print(tokens_example2)\n","#print(mytokenizer.get_other_features())\n","#print(len(mytokenizer.get_other_features()[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pk8OZ0hlaGva","colab_type":"text"},"source":["Read data"]},{"cell_type":"code","metadata":{"id":"swwE1I3GaJeL","colab_type":"code","outputId":"cd236470-555d-41eb-bcbf-384664a09022","executionInfo":{"status":"ok","timestamp":1568270120198,"user_tz":-600,"elapsed":1266,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["from sklearn.model_selection import train_test_split\n","\n","X = []\n","Y = []\n","with open ('/content/gdrive/My Drive/SML_Project1/train_tweets.txt') as fp:\n","    for line in fp:\n","        data = line.split(\"\\t\")\n","        X.append(data[1])\n","        Y.append(int(data[0]))\n","\n","X_test = []\n","with open ('/content/gdrive/My Drive/SML_Project1/test_tweets_unlabeled.txt') as fp:\n","    for line in fp:\n","        X_test.append(line)\n","    \n","print(len(X))\n","print(len(Y))\n","print(len(X_final_test))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["328932\n","328932\n","35437\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-juY8t7Lak7s","colab_type":"text"},"source":["Tokenize X_train, X_test"]},{"cell_type":"code","metadata":{"id":"dSyQdbIvatM9","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","X_train_tokens = []\n","X_test_tokens = []\n","\n","for tweet in X:\n","    X_train_tokens.append(mytokenizer.tokenize(tweet))\n","for tweet in X_test:\n","    X_test_tokens.append(mytokenizer.tokenize(tweet))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xljq0Y06Jm8I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"62eacc8d-b29e-4b0c-d7dc-c56aa4f793f8","executionInfo":{"status":"ok","timestamp":1568270397896,"user_tz":-600,"elapsed":3186,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}}},"source":["vocab = mytokenizer.get_vocab()\n","print(len(vocab))\n","\n","def token2seq(tokens_list):\n","    seqs_list = []\n","    for tokens in tokens_list:\n","        seqs = []\n","        for token in tokens:\n","            seqs.append(vocab[token])\n","        seqs_list.append(seqs)\n","    return seqs_list\n","\n","X_train_tokens = token2seq(X_train_tokens)\n","X_test_tokens = token2seq(X_test_tokens)\n","\n","print(len(X_train_tokens))\n","print(len(X_test_tokens))\n","print(X_train_tokens[34])"],"execution_count":44,"outputs":[{"output_type":"stream","text":["158974\n","328932\n","35437\n","[39, 22, 27, 231, 232, 233, 92, 69, 234, 235, 13, 236, 86, 39, 237, 106]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KNHz_8dxMYkJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"808fac47-b778-40c3-da5b-0fdb4bb51089","executionInfo":{"status":"ok","timestamp":1568270401796,"user_tz":-600,"elapsed":2404,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}}},"source":["from keras.preprocessing.sequence import pad_sequences\n","max_length = mytokenizer.get_max_seq()\n","print(max_length)\n","padding = 0\n","X_train_tokens = pad_sequences(X_train_tokens, dtype=int, maxlen=max_length, value=padding)\n","X_test_tokens = pad_sequences(X_test_tokens, dtype=int, maxlen=max_length, value=padding)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["118\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ghONM3eA0li","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import GaussianNB\n","clf = GaussianNB()\n","clf.fit(X_train_tokens, Y)\n","Y_final_test = clf.predict(X_test_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HMNmtYgN98P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bc3cb043-df82-4bb5-ddc8-f6f45bcb1f88","executionInfo":{"status":"ok","timestamp":1568270718012,"user_tz":-600,"elapsed":2092,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}}},"source":["print(len(Y_final_test))"],"execution_count":48,"outputs":[{"output_type":"stream","text":["35437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zNlMMntx2CdB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4d83cf0e-c3fb-47de-fda8-3850789f2825","executionInfo":{"status":"ok","timestamp":1568270730195,"user_tz":-600,"elapsed":2065,"user":{"displayName":"杨帆","photoUrl":"","userId":"07701850902501518716"}}},"source":["import pandas as pd\n","output_file = \"/content/gdrive/My Drive/SML_Project1/pridicted_NB.csv\"\n","df = pd.DataFrame({'Id': np.arange(1, len(Y_final_test) + 1),\n","                  'Predicted' : np.array(Y_final_test)})\n","print(df)\n","df.to_csv(output_file,index=False)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["          Id  Predicted\n","0          1       3763\n","1          2        935\n","2          3       8620\n","3          4       7254\n","4          5       8357\n","5          6       6074\n","6          7       1059\n","7          8       6664\n","8          9       6873\n","9         10       9566\n","10        11       3854\n","11        12       8706\n","12        13       4486\n","13        14       8481\n","14        15       3854\n","15        16       8490\n","16        17       3316\n","17        18       1997\n","18        19       6531\n","19        20       4837\n","20        21       3480\n","21        22       2019\n","22        23       4650\n","23        24       5132\n","24        25       5809\n","25        26       6490\n","26        27       5362\n","27        28       7561\n","28        29        369\n","29        30       6225\n","...      ...        ...\n","35407  35408       2489\n","35408  35409       3091\n","35409  35410       8796\n","35410  35411       8353\n","35411  35412       5458\n","35412  35413       1237\n","35413  35414       5528\n","35414  35415       5034\n","35415  35416       6362\n","35416  35417       7904\n","35417  35418       2188\n","35418  35419       9578\n","35419  35420       8605\n","35420  35421       7527\n","35421  35422       4351\n","35422  35423       4461\n","35423  35424       2262\n","35424  35425       2885\n","35425  35426       1222\n","35426  35427       8876\n","35427  35428       4080\n","35428  35429        434\n","35429  35430       1820\n","35430  35431       1100\n","35431  35432       2188\n","35432  35433       1820\n","35433  35434       5306\n","35434  35435        389\n","35435  35436       3985\n","35436  35437       2019\n","\n","[35437 rows x 2 columns]\n"],"name":"stdout"}]}]}